# LightRAG Gemini 2.0 Flash Production Pipeline - Comprehensive Documentation

## üåü Overview

This document provides complete documentation for the LightRAG Gemini 2.0 Flash Production Pipeline - a robust, production-ready Retrieval-Augmented Generation (RAG) system built with:

- **Gemini 2.0 Flash LLM** for advanced language understanding and generation
- **Gemini text-embedding-001** for high-dimensional (3072D) vector embeddings
- **Gemma tokenizer** for accurate token counting and management
- **LLM-based reranking** for intelligent document prioritization
- **Streamlit interface** with advanced chat, knowledge graph visualization, and monitoring
- **LightRAG workspace system** for data organization and isolation

---

## üìÅ System Architecture

### Core Components

```
LightRAG-Gemini-Pipeline/
‚îú‚îÄ‚îÄ üîß Core Configuration
‚îÇ   ‚îú‚îÄ‚îÄ .env                          # Complete environment configuration
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ ü§ñ AI Integration
‚îÇ   ‚îú‚îÄ‚îÄ gemini_llm.py                 # Gemini 2.0 Flash LLM implementation
‚îÇ   ‚îú‚îÄ‚îÄ gemini_embeddings.py          # Gemini embeddings (3072D vectors)
‚îÇ   ‚îú‚îÄ‚îÄ gemma_tokenizer.py            # Gemma tokenizer implementation
‚îÇ   ‚îî‚îÄ‚îÄ llm_reranker.py               # LLM-based intelligent reranking
‚îú‚îÄ‚îÄ üöÄ Server & API
‚îÇ   ‚îú‚îÄ‚îÄ lightrag_gemini_server.py     # FastAPI production server
‚îÇ   ‚îî‚îÄ‚îÄ start_server.py               # Server startup script
‚îú‚îÄ‚îÄ üñ•Ô∏è User Interface
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py              # Advanced Streamlit interface
‚îú‚îÄ‚îÄ üìö LightRAG Source
‚îÇ   ‚îî‚îÄ‚îÄ LightRAG/                     # LightRAG core framework (built from source)
‚îî‚îÄ‚îÄ üìñ Documentation
    ‚îú‚îÄ‚îÄ README.md                     # Quick start guide
    ‚îî‚îÄ‚îÄ PIPELINE_DOCUMENTATION.md    # This comprehensive guide
```

---

## üîß File-by-File Documentation

### üî© Core Configuration Files

#### `.env` - Complete Environment Configuration
**Purpose**: Centralized configuration for all system components

**Key Sections**:
- **Server Configuration**: Host, port, CORS settings
- **Gemini LLM Settings**: API keys, model parameters, safety settings
- **Embedding Configuration**: Model selection, dimensions, batch processing
- **Reranking Setup**: LLM-based reranking parameters
- **Document Processing**: Chunking, summarization, gleaning settings
- **LightRAG Workspace**: Default workspace configuration and data isolation
- **Streamlit UI Settings**: Default parameters, visualization options
- **Performance Tuning**: Concurrency, caching, optimization settings
- **Monitoring & Analytics**: Token tracking, metrics, health checks

**Environment Variables** (Key Examples):
```bash
# Gemini Configuration
GEMINI_API_KEY=your_gemini_api_key_here
LLM_MODEL=gemini-2.0-flash
EMBEDDING_MODEL=text-embedding-001
EMBEDDING_DIM=3072

# LLM-based Reranking
ENABLE_RERANK=true
RERANK_MODE=llm
RERANK_LLM_MODEL=gemini-2.0-flash

# Streamlit Defaults
DEFAULT_QUERY_MODE=mix
DEFAULT_TOP_K=10
DEFAULT_TEMPERATURE=0.7
KG_DEFAULT_MAX_NODES=500

# Workspace Management
WORKSPACE=gemini_production
WORKSPACE_AUTO_CREATE=true
WORKSPACE_BACKUP_ENABLED=true
```

#### `requirements.txt` - Python Dependencies
**Purpose**: Defines all required Python packages

**Key Dependencies**:
- `fastapi>=0.104.1` - API server framework
- `streamlit>=1.28.0` - Interactive web interface
- `google-genai` - Gemini API integration
- `sentencepiece` - Gemma tokenizer support
- `networkx` - Knowledge graph processing
- `plotly` - Interactive visualizations
- `pandas` - Data processing
- `docling` - Advanced document parsing

**Note**: LightRAG is built from source (no pip dependency)

### ü§ñ AI Integration Components

#### `gemini_llm.py` - Gemini 2.0 Flash LLM Implementation
**Purpose**: Production-ready Gemini LLM integration with advanced features

**Key Classes**:
- `GeminiConfig` - Configuration management with environment loading
- `GeminiLLM` - Main LLM class with token tracking and error handling
- `SafetySetting` - Gemini safety configuration enum

**Features**:
- Asynchronous text generation with streaming support
- Comprehensive token tracking and cost monitoring
- Robust error handling with retry logic
- Safety settings and content filtering
- Multiple output format support (text, JSON, structured)
- Performance optimization with configurable timeouts

**Configuration Sources**: All settings loaded from environment variables
- API keys, model selection, temperature, token limits
- Safety settings, retry policies, performance tuning

#### `gemini_embeddings.py` - Gemini Text Embeddings
**Purpose**: High-dimensional (3072D) vector embeddings using Gemini's native embedding model

**Key Classes**:
- `GeminiEmbeddingConfig` - Configuration with environment integration
- `GeminiEmbeddings` - Embedding generation with caching and batching

**Features**:
- **text-embedding-001** model integration (3072 dimensions)
- Intelligent batching for performance optimization
- Comprehensive caching system to reduce API calls
- Similarity computation and vector operations
- Error handling and fallback mechanisms
- Memory-efficient processing for large document sets

**Performance Optimizations**:
- Configurable batch sizes (default: 32)
- Request rate limiting and queue management
- Vector similarity caching

#### `gemma_tokenizer.py` - Gemma Tokenizer Implementation
**Purpose**: Accurate token counting for Gemini models using Gemma tokenizer

**Key Classes**:
- `GemmaTokenizer` - LightRAG-compatible tokenizer implementation

**Features**:
- SentencePiece model integration for Gemma
- Accurate token counting for cost estimation
- Text encoding/decoding capabilities
- Integration with LightRAG's chunking system
- Configurable model download and caching

**Integration Points**:
- Document chunking and splitting
- Token budget management for LLM calls
- Cost estimation and monitoring

#### `llm_reranker.py` - LLM-Based Intelligent Reranking
**Purpose**: Advanced document reranking using Gemini LLM intelligence

**Key Classes**:
- `LLMRerankerConfig` - Configuration management
- `LLMReranker` - Main reranking implementation

**Features**:
- **LLM-based ranking**: Uses Gemini to intelligently rank documents
- **Semantic understanding**: Goes beyond keyword matching
- **Batch processing**: Efficient handling of large document sets
- **Relevance scoring**: Provides confidence scores for rankings
- **Fallback handling**: Graceful degradation on errors

**Ranking Process**:
1. Document truncation for efficiency
2. Intelligent prompt construction
3. LLM-based relevance analysis
4. JSON response parsing and validation
5. Score assignment and reordering

### üöÄ Server & API Components

#### `lightrag_gemini_server.py` - FastAPI Production Server
**Purpose**: Production-ready API server exposing all LightRAG functionality

**Key Components**:
- `ServerConfig` - Environment-based configuration management
- `create_app()` - FastAPI application factory
- Router integration for modular API structure

**API Endpoints**:
- **Query Endpoints**: `/query`, `/query/stream` - RAG querying with all modes
- **Document Management**: `/documents/upload`, `/documents/text`, `/documents/scan`
- **Graph Operations**: `/graph/graphs`, `/graph/entity/exists`, `/graph/entity/edit`
- **System Monitoring**: `/health`, `/metrics`, `/pipeline_status`
- **Cache Management**: `/documents/clear_cache`, `/delete_entity`, `/delete_relation`

**Production Features**:
- **REST API** with comprehensive endpoints
- **Streamlit interface** with chat, knowledge graph visualization, and dashboards
- **Health monitoring** and metrics collection
- **Configurable storage** backends (JSON, Redis, Neo4j, PostgreSQL, etc.)
- **Production deployment** ready configuration

**Integration Points**:
- LightRAG core framework (built from source)
- Gemini LLM and embedding functions
- LLM-based reranking system
- LightRAG workspace system for data isolation

#### `start_server.py` - Server Startup Script
**Purpose**: Simplified server startup with dependency checking

**Features**:
- Environment validation and setup
- Dependency verification
- Graceful error handling for missing components
- Development vs production startup modes
- Logging configuration
- Workspace initialization

### üñ•Ô∏è User Interface

#### `streamlit_app.py` - Advanced Streamlit Interface
**Purpose**: Comprehensive web interface with chat, visualization, and monitoring

**Key Features**:

##### üí¨ **Intelligent Chat Interface**
- **Configurable Parameters**: Real-time adjustment of query parameters
  - Query modes (mix, local, global, hybrid, naive)
  - Top-K settings for entities and chunks
  - Temperature control for response randomness
  - Token limits and history management
  - Reranking and streaming toggles
- **Response Metadata**: Detailed information about each query
  - Processing time, token usage, source count
  - Mode used, reranking status, entity/relation counts
- **Chat History**: Persistent conversation with pagination
- **Parameter Visualization**: Current configuration display

##### üï∏Ô∏è **Knowledge Graph Visualizer**
- **Interactive Exploration**: Dynamic graph visualization with Plotly
- **Multiple Layouts**: Spring, circular, Kamada-Kawai, random algorithms
- **Configurable Parameters**:
  - Traversal depth (1-10 hops)
  - Node limits (50-2000 for performance)
  - Visual customization (colors, sizes)
- **Graph Analytics**:
  - Node/edge statistics
  - Centrality measures
  - Connected components analysis
  - Density and path length metrics
- **Export Options**: JSON, CSV, HTML formats for external analysis

##### üìã **Document Management Dashboard**
- **File Upload**: Multi-format support (PDF, DOCX, PPTX, TXT, MD, JSON, CSV, XLSX)
- **Text Input**: Direct text addition with source tracking
- **Processing Status**: Real-time monitoring with status distribution charts
- **Management Actions**: Scan, refresh, cache clearing, document deletion
- **Filtering & Search**: Advanced document filtering and pagination

##### üìà **System Monitoring Dashboard**
- **Performance Metrics**: Real-time system statistics
  - Uptime, request counts, document statistics
  - Token usage analytics with cost estimation
- **API Analytics**: Comprehensive API call tracking
  - Success rates, response times, endpoint usage
  - Timeline visualization and performance analysis
- **Configuration Display**: Current system settings
- **Auto-refresh**: Configurable dashboard updates

**Environment Integration**: All defaults loaded from environment variables
- UI settings, query parameters, visualization options
- API connection details, timeout configurations
- Performance and display preferences

### üîÑ Workspace Management

**LightRAG Default Workspace System**: The system uses LightRAG's built-in workspace mechanisms for data organization and management.

**Configuration**:
- `WORKSPACE` - Workspace name for data isolation
- `WORKING_DIR` - Base directory for all workspace data

**Features**:
- **Automatic Directory Creation**: LightRAG creates necessary directories automatically
- **Data Isolation**: Each workspace maintains separate data stores
- **Storage Organization**: Organized structure for different data types
  - Vector embeddings storage
  - Knowledge graph data
  - Document status tracking
  - Key-value storage
- **Multi-tenant Support**: Multiple workspaces for different use cases

---

## üöÄ Running the System

### 1. Initial Setup

#### Prerequisites
```bash
# Python 3.8+
python --version

# Install dependencies
pip install -r requirements.txt

# Get Gemini API key from Google AI Studio
# https://makersuite.google.com/app/apikey
```

#### Environment Configuration
```bash
# Copy and configure environment file
cp .env.example .env

# Required settings (minimum)
GEMINI_API_KEY=your_gemini_api_key_here
WORKSPACE=your_workspace_name
```

### 2. Starting the API Server

#### Development Mode
```bash
# Start with auto-reload
python start_server.py --dev

# Or directly with uvicorn
uvicorn lightrag_gemini_server:app --reload --host 0.0.0.0 --port 9621
```

#### Production Mode
```bash
# Start production server
python start_server.py --production

# With gunicorn (recommended for production)
gunicorn lightrag_gemini_server:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:9621
```

**Server Endpoints**:
- **API Documentation**: http://localhost:9621/docs
- **Health Check**: http://localhost:9621/health
- **Metrics**: http://localhost:9621/metrics

### 3. Starting the Streamlit Interface

#### Basic Startup
```bash
# Start Streamlit app
streamlit run streamlit_app.py

# Custom host and port
streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

#### Advanced Configuration
```bash
# With custom configuration
STREAMLIT_PORT=8502 STREAMLIT_HOST=localhost streamlit run streamlit_app.py
```

**Interface Access**:
- **Main Interface**: http://localhost:8501
- **Chat**: Interactive conversation with RAG system
- **Knowledge Graph**: Visual graph exploration
- **Documents**: File management and processing status
- **Monitoring**: System metrics and API analytics

### 4. Production Deployment

#### Docker Deployment (Recommended)
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 9621 8501

CMD ["python", "start_server.py", "--production"]
```

#### Cloud Run Deployment
```yaml
service: lightrag-gemini
runtime: python311
instance_class: F4_1G

env_variables:
  GEMINI_API_KEY: your_key_here
  HOST: 0.0.0.0
  PORT: 8080
  WORKSPACE: production

automatic_scaling:
  min_instances: 1
  max_instances: 10
```

---

## üéØ Core Functionalities

### üìñ Document Processing

#### Supported Formats
- **Text Files**: TXT, MD, JSON, CSV
- **Office Documents**: PDF, DOCX, PPTX, XLSX
- **Web Content**: HTML, XML
- **Code**: Various programming languages

#### Processing Pipeline
1. **Document Ingestion**: File upload or text input
2. **Format Detection**: Automatic content type recognition
3. **Text Extraction**: Advanced parsing with Docling
4. **Chunking**: Intelligent text segmentation with overlap
5. **Entity Extraction**: Knowledge graph entity identification
6. **Embedding Generation**: 3072D vector embeddings
7. **Graph Construction**: Relationship mapping and storage
8. **Status Tracking**: Real-time processing status updates

#### Configuration Options
```bash
# Document processing settings
CHUNK_SIZE=1200
CHUNK_OVERLAP_SIZE=200
MAX_GLEANING=2
DOCUMENT_LOADING_ENGINE=DOCLING
SUMMARY_LANGUAGE=English
```

### üîç Query & Retrieval

#### Query Modes
- **Mix Mode**: Combines local and global retrieval strategies
- **Local Mode**: Focuses on specific document chunks
- **Global Mode**: Uses knowledge graph relationships
- **Hybrid Mode**: Balances chunk-based and graph-based retrieval
- **Naive Mode**: Simple similarity-based retrieval

#### Retrieval Process
1. **Query Analysis**: Intent understanding and preprocessing
2. **Entity Identification**: Relevant entity extraction from query
3. **Multi-mode Retrieval**: Parallel execution of retrieval strategies
4. **Result Aggregation**: Combining results from different modes
5. **LLM-based Reranking**: Intelligent result prioritization
6. **Context Assembly**: Preparing context for LLM generation
7. **Response Generation**: Gemini-powered answer synthesis

#### Configurable Parameters
- **Top-K**: Number of entities/relations to retrieve (1-100)
- **Chunk Top-K**: Number of document chunks to consider (1-50)
- **Temperature**: Response randomness control (0.0-2.0)
- **Max Tokens**: Output length limits (256-8192)
- **History Turns**: Conversation context length (0-20)
- **Reranking**: Enable/disable LLM-based reranking

### üß† Knowledge Graph

#### Graph Construction
- **Entity Extraction**: Advanced NLP for entity identification
- **Relationship Discovery**: Automated relationship inference
- **Graph Storage**: Efficient NetworkX-based storage
- **Incremental Updates**: Dynamic graph expansion

#### Visualization Features
- **Interactive Exploration**: Click and drag interface
- **Multiple Layouts**: Various visualization algorithms
- **Filtering & Search**: Dynamic graph subset exploration
- **Analytics**: Centrality measures, clustering analysis
- **Export Options**: Multiple format support for external tools

#### Graph Operations
- **Entity Management**: Add, edit, delete entities
- **Relationship Editing**: Modify connection strengths and types
- **Subgraph Extraction**: Focus on specific graph regions
- **Path Finding**: Discover connections between entities

### üîÑ Reranking System

#### LLM-based Reranking
- **Intelligent Analysis**: Gemini-powered relevance assessment
- **Semantic Understanding**: Beyond keyword matching
- **Context Awareness**: Query-specific relevance scoring
- **Batch Processing**: Efficient handling of multiple documents
- **Fallback Mechanisms**: Graceful degradation on errors

#### Configuration
```bash
# Reranking settings
ENABLE_RERANK=true
RERANK_MODE=llm
RERANK_LLM_MODEL=gemini-2.0-flash
RERANK_MAX_DOCS=20
RERANK_TEMPERATURE=0.1
```

### üíæ Storage & Caching

#### Default Storage (Development)
- **JsonKVStorage**: Key-value data in JSON files
- **NanoVectorDBStorage**: Lightweight vector storage
- **NetworkXStorage**: Graph data in NetworkX format
- **JsonDocStatusStorage**: Document status tracking

#### Production Storage Options
```bash
# Redis for caching and KV storage
LIGHTRAG_KV_STORAGE=RedisKVStorage
REDIS_URI=redis://localhost:6379

# Milvus for vector storage
LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
MILVUS_URI=http://localhost:19530

# Neo4j for graph storage
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io

# PostgreSQL for structured data
POSTGRES_HOST=localhost
POSTGRES_DATABASE=lightrag_db
```

#### Caching Strategy
- **LLM Response Caching**: Reduce API calls for repeated queries
- **Embedding Caching**: Store computed embeddings
- **Graph Caching**: Cache frequently accessed graph sections
- **Query Result Caching**: Cache complex query results

---

## üîß Configuration Guide

### Environment Variables Reference

#### Core System Settings
```bash
# Server Configuration
HOST=0.0.0.0                    # Server host address
PORT=9621                       # Server port
WORKERS=1                       # Number of worker processes
CORS_ORIGINS=*                  # CORS allowed origins

# Workspace Configuration (LightRAG Default)
WORKSPACE=gemini_production     # Workspace name for data isolation
WORKING_DIR=./rag_storage      # Base storage directory
```

#### Gemini Configuration
```bash
# LLM Settings
LLM_BINDING=gemini
LLM_MODEL=gemini-2.0-flash
GEMINI_API_KEY=your_key_here
GEMINI_BASE_URL=https://generativelanguage.googleapis.com
TEMPERATURE=0.1
TOP_K_SAMPLING=40
TOP_P_SAMPLING=0.9
GEMINI_MAX_OUTPUT_TOKENS=8192

# Embedding Settings
EMBEDDING_BINDING=gemini
EMBEDDING_MODEL=text-embedding-001
EMBEDDING_DIM=3072
EMBEDDING_BATCH_SIZE=32
EMBEDDING_API_KEY=your_key_here

# Safety Settings
GEMINI_SAFETY_HARASSMENT=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_HATE_SPEECH=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_SEXUALLY_EXPLICIT=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_DANGEROUS_CONTENT=BLOCK_MEDIUM_AND_ABOVE
```

#### Query & Retrieval Settings
```bash
# Query Configuration
TOP_K=60                        # Entities/relations to retrieve
CHUNK_TOP_K=15                  # Chunks to send to LLM
MAX_ENTITY_TOKENS=12000         # Entity token limit
MAX_RELATION_TOKENS=12000       # Relation token limit
MAX_TOTAL_TOKENS=32000          # Total token limit
COSINE_THRESHOLD=0.2            # Similarity threshold
HISTORY_TURNS=3                 # Conversation history

# Reranking Configuration
ENABLE_RERANK=true
RERANK_MODE=llm
RERANK_LLM_MODEL=gemini-2.0-flash
RERANK_MAX_DOCS=20
RERANK_TEMPERATURE=0.1
```

#### Document Processing
```bash
# Text Processing
CHUNK_SIZE=1200                 # Document chunk size
CHUNK_OVERLAP_SIZE=200          # Chunk overlap
MAX_GLEANING=2                  # Entity extraction attempts
DOCUMENT_LOADING_ENGINE=DOCLING # Document parser
SUMMARY_LANGUAGE=English        # Summary language

# Concurrency Settings
MAX_ASYNC=6                     # Max LLM concurrent requests
MAX_PARALLEL_INSERT=3           # Parallel document processing
EMBEDDING_FUNC_MAX_ASYNC=8      # Max embedding requests
EMBEDDING_BATCH_NUM=16          # Embedding batch size
```

#### Streamlit Interface Settings
```bash
# Streamlit Configuration
STREAMLIT_HOST=localhost
STREAMLIT_PORT=8501
STREAMLIT_TITLE=LightRAG Gemini 2.0 Flash
STREAMLIT_PAGE_ICON=üß†
STREAMLIT_LAYOUT=wide

# Default UI Parameters
DEFAULT_QUERY_MODE=mix
DEFAULT_TOP_K=10
DEFAULT_CHUNK_TOP_K=5
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_OUTPUT_TOKENS=2048
DEFAULT_ENABLE_RERANK=true
DEFAULT_STREAM_RESPONSE=true

# Knowledge Graph Visualization
KG_DEFAULT_MAX_DEPTH=3
KG_DEFAULT_MAX_NODES=500
KG_LAYOUT_ALGORITHM=spring_layout
KG_NODE_SIZE=20
KG_EDGE_WIDTH=1
KG_NODE_COLOR=lightblue
KG_EDGE_COLOR=gray
KG_FIGURE_HEIGHT=600

# UI Performance
UI_AUTO_REFRESH_INTERVAL=30
UI_MAX_CHAT_HISTORY=100
UI_PAGE_SIZE=50
UI_CHART_HEIGHT=400
```

#### Performance & Monitoring
```bash
# Caching
ENABLE_LLM_CACHE=true
LLM_CACHE_TTL=3600
LLM_CACHE_MAX_SIZE=10000
EMBEDDING_CACHE_ENABLED=true
EMBEDDING_CACHE_SIMILARITY_THRESHOLD=0.95

# Monitoring
ENABLE_TOKEN_TRACKING=true
TOKEN_TRACKING_LOG_INTERVAL=100
ENABLE_METRICS=true
METRICS_ENDPOINT=/metrics
HEALTH_CHECK_INTERVAL=60

# Performance Tuning
MEMORY_OPTIMIZATION=true
BATCH_PROCESSING=true
INDEX_BATCH_SIZE=100
INDEX_PARALLEL_WORKERS=4
```

---

## üîç Troubleshooting Guide

### Common Issues & Solutions

#### 1. Server Startup Issues
**Problem**: Server fails to start
**Solutions**:
```bash
# Check environment configuration
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('GEMINI_API_KEY:', bool(os.getenv('GEMINI_API_KEY')))"

# Verify dependencies
pip install -r requirements.txt

# Check port availability
netstat -an | grep 9621

# Start with debug mode
python start_server.py --debug
```

#### 2. Gemini API Issues
**Problem**: API quota or rate limit errors
**Solutions**:
```bash
# Verify API key is set
echo $GEMINI_API_KEY

# Check quota limits in Google AI Studio
# Enable billing if necessary
# Monitor usage in the dashboard
```

#### 3. Memory Issues
**Problem**: High memory usage or OOM errors
**Solutions**:
```bash
# Reduce batch sizes
EMBEDDING_BATCH_SIZE=16
CHUNK_TOP_K=10
MAX_ASYNC=3

# Enable memory optimization
MEMORY_OPTIMIZATION=true

# Use production storage backends
LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
```

#### 4. Workspace Issues
**Problem**: Workspace creation or access errors
**Solutions**:
```bash
# Check permissions
ls -la ./rag_storage/

# Reset workspace
rm -rf ./rag_storage/your_workspace

# Check disk space
df -h

# Check workspace directory
ls -la ./rag_storage/$WORKSPACE
```

#### 5. Streamlit Interface Issues
**Problem**: UI not loading or API connection errors
**Solutions**:
```bash
# Check API server status
curl http://localhost:9621/health

# Verify Streamlit configuration
streamlit run streamlit_app.py --server.headless true

# Check browser console for errors
# Clear browser cache
# Try different port: streamlit run streamlit_app.py --server.port 8502
```

### Performance Optimization

#### 1. Query Performance
- **Adjust Top-K values**: Lower values for faster queries
- **Enable caching**: LLM and embedding caching
- **Use reranking judiciously**: Balance quality vs speed
- **Optimize chunk sizes**: Balance context vs processing time

#### 2. Document Processing
- **Batch processing**: Enable parallel document insertion
- **Efficient chunking**: Optimize chunk size and overlap
- **Storage optimization**: Use appropriate storage backends
- **Memory management**: Monitor and optimize memory usage

#### 3. System Scaling
- **Horizontal scaling**: Multiple server instances with load balancer
- **Database scaling**: Use production databases (Redis, Milvus, Neo4j)
- **Caching layers**: Redis for distributed caching
- **API rate limiting**: Protect against overuse

---

## üìà Monitoring & Analytics

### System Metrics
- **Server Health**: Uptime, response times, error rates
- **Token Usage**: Cost monitoring and optimization
- **Query Performance**: Response times by mode
- **Document Processing**: Throughput and success rates
- **Storage Usage**: Workspace size and growth trends

### API Analytics
- **Endpoint Usage**: Most used API endpoints
- **Success Rates**: Error tracking and debugging
- **Response Times**: Performance monitoring
- **User Patterns**: Usage analytics and optimization

### Business Metrics
- **User Engagement**: Query frequency and patterns
- **Content Quality**: Document processing success rates
- **System Adoption**: Feature usage analytics
- **Cost Optimization**: Token usage and cost tracking

---

## üõ°Ô∏è Security & Best Practices

### API Security
- **Input Validation**: Sanitize all user inputs
- **CORS Configuration**: Restrict cross-origin requests
- **Rate Limiting**: Prevent abuse and overuse (can be added if needed)

### Data Protection
- **Access Control**: Workspace-based data isolation
- **Audit Logging**: Track all system access
- **Environment Variables**: Secure API key storage

### Production Deployment
- **Environment Separation**: Dev/staging/prod environments
- **Secrets Management**: Secure API key storage
- **Container Security**: Secure Docker images
- **Monitoring**: Comprehensive system monitoring

---

## üöÄ Advanced Usage Patterns

### Multi-tenant Deployment
```bash
# Workspace isolation
WORKSPACE=tenant_1
# Deploy separate instances for each tenant
```

### High-availability Setup
```bash
# Load balancer configuration
# Multiple server instances
# Database clustering
# Redis cluster for caching
```

### Integration Examples
```python
# Python SDK usage
from lightrag import LightRAG
from gemini_llm import GeminiConfig, gemini_model_complete
from gemini_embeddings import gemini_embed

# Initialize RAG system
rag = LightRAG(
    llm_model_func=gemini_model_complete,
    embedding_func=gemini_embed,
    working_dir="./custom_workspace"
)

# Add documents
await rag.ainsert("Your document content here")

# Query
result = await rag.aquery("Your question here", mode="mix")
```

---